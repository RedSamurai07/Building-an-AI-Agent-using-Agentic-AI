{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91457f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import yfinance as yf\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445b364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# define stock symbol and time period\n",
    "symbol = \"AMZN\" # Microsoft stock price\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "\n",
    "# download historical data\n",
    "data = yf.download(symbol, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afd45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "data['Returns'] = data['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3574a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN values and reset index\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d8842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = {0: \"HOLD\", 1: \"BUY\", 2: \"SELL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1158a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state function\n",
    "def get_state(data, index):\n",
    "    return np.array([\n",
    "        float(data.loc[index, 'Close']),\n",
    "        float(data.loc[index, 'SMA_5']),\n",
    "        float(data.loc[index, 'SMA_20']),\n",
    "        float(data.loc[index, 'Returns'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f257c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading environment\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.initial_balance = 10000\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0\n",
    "        self.index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0\n",
    "        self.index = 0\n",
    "        return get_state(self.data, self.index)\n",
    "\n",
    "    def step(self, action):\n",
    "        price = float(self.data.loc[self.index, 'Close'])\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1 and self.balance >= price:  # BUY\n",
    "            self.holdings = self.balance // price\n",
    "            self.balance -= self.holdings * price\n",
    "        elif action == 2 and self.holdings > 0:  # SELL\n",
    "            self.balance += self.holdings * price\n",
    "            self.holdings = 0\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.data) - 1\n",
    "\n",
    "        if done:\n",
    "            reward = self.balance - self.initial_balance\n",
    "\n",
    "        next_state = get_state(self.data, self.index) if not done else None\n",
    "        return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac73445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f93e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(list(ACTIONS.keys()))\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                target += self.gamma * torch.max(self.model(next_state_tensor)).item()\n",
    "\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            target_tensor = self.model(state_tensor).clone().detach()\n",
    "            target_tensor[0][action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state_tensor)\n",
    "            loss = self.criterion(output, target_tensor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c25218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: -9756.88996887207\n",
      "Episode 2/500, Total Reward: -9774.258399963379\n",
      "Episode 3/500, Total Reward: -9879.021842956543\n",
      "Episode 4/500, Total Reward: 4522.2707443237305\n",
      "Episode 5/500, Total Reward: -9968.335319519043\n",
      "Episode 6/500, Total Reward: -9906.981742858887\n",
      "Episode 7/500, Total Reward: -9849.928848266602\n",
      "Episode 8/500, Total Reward: -9885.15975189209\n",
      "Episode 9/500, Total Reward: -9864.92121887207\n",
      "Episode 10/500, Total Reward: -9845.742446899414\n",
      "Episode 11/500, Total Reward: -9992.506126403809\n",
      "Episode 12/500, Total Reward: -9738.810195922852\n",
      "Episode 13/500, Total Reward: -9736.839126586914\n",
      "Episode 14/500, Total Reward: -9822.716102600098\n",
      "Episode 15/500, Total Reward: 11332.127960205078\n",
      "Episode 16/500, Total Reward: -9794.054298400879\n",
      "Episode 17/500, Total Reward: -9998.553848266602\n",
      "Episode 18/500, Total Reward: -9818.315307617188\n",
      "Episode 19/500, Total Reward: -9987.6556930542\n",
      "Episode 20/500, Total Reward: -9811.091979980469\n",
      "Episode 21/500, Total Reward: -9949.742637634277\n",
      "Episode 22/500, Total Reward: -9822.379760742188\n",
      "Episode 23/500, Total Reward: -9876.519309997559\n",
      "Episode 24/500, Total Reward: -9807.010070800781\n",
      "Episode 25/500, Total Reward: -9976.054557800293\n",
      "Episode 26/500, Total Reward: -9737.088363647461\n",
      "Episode 27/500, Total Reward: -9895.901748657227\n",
      "Episode 28/500, Total Reward: -2557.390426635742\n",
      "Episode 29/500, Total Reward: -9868.696418762207\n",
      "Episode 30/500, Total Reward: -9909.86833190918\n",
      "Episode 31/500, Total Reward: -9843.724815368652\n",
      "Episode 32/500, Total Reward: -9877.620780944824\n",
      "Episode 33/500, Total Reward: -9904.09302520752\n",
      "Episode 34/500, Total Reward: -9786.584373474121\n",
      "Episode 35/500, Total Reward: -9776.359802246094\n",
      "Episode 36/500, Total Reward: -2017.1695404052734\n",
      "Episode 37/500, Total Reward: -9780.652954101562\n",
      "Episode 38/500, Total Reward: -9779.055694580078\n",
      "Episode 39/500, Total Reward: -9858.675064086914\n",
      "Episode 40/500, Total Reward: -9968.487846374512\n",
      "Episode 41/500, Total Reward: -9720.1103515625\n",
      "Episode 42/500, Total Reward: -9844.4439163208\n",
      "Episode 43/500, Total Reward: -9807.768859863281\n",
      "Episode 44/500, Total Reward: 7656.960838317871\n",
      "Episode 45/500, Total Reward: -9889.001792907715\n",
      "Episode 46/500, Total Reward: -9980.000144958496\n",
      "Episode 47/500, Total Reward: 3700.0093154907227\n",
      "Episode 48/500, Total Reward: -9840.716926574707\n",
      "Episode 49/500, Total Reward: -9959.459945678711\n",
      "Episode 50/500, Total Reward: -9910.905281066895\n",
      "Episode 51/500, Total Reward: -9879.799392700195\n",
      "Episode 52/500, Total Reward: -9979.686462402344\n",
      "Episode 53/500, Total Reward: -9794.218154907227\n",
      "Episode 54/500, Total Reward: -9792.130920410156\n",
      "Episode 55/500, Total Reward: -9812.909523010254\n",
      "Episode 56/500, Total Reward: -9773.549369812012\n",
      "Episode 57/500, Total Reward: -9945.417930603027\n",
      "Episode 58/500, Total Reward: -9952.91512298584\n",
      "Episode 59/500, Total Reward: -9883.423599243164\n",
      "Episode 60/500, Total Reward: 1482.4067611694336\n",
      "Episode 61/500, Total Reward: -9867.742073059082\n",
      "Episode 62/500, Total Reward: -9879.209815979004\n",
      "Episode 63/500, Total Reward: -9832.643112182617\n",
      "Episode 64/500, Total Reward: -9872.041847229004\n",
      "Episode 65/500, Total Reward: -9889.281547546387\n",
      "Episode 66/500, Total Reward: -9896.251426696777\n",
      "Episode 67/500, Total Reward: -9873.133430480957\n",
      "Episode 68/500, Total Reward: -9796.773887634277\n",
      "Episode 69/500, Total Reward: -9875.54940032959\n",
      "Episode 70/500, Total Reward: -9771.678916931152\n",
      "Episode 71/500, Total Reward: -9998.46997833252\n"
     ]
    }
   ],
   "source": [
    "# train the agent\n",
    "env = TradingEnvironment(data)\n",
    "agent = DQNAgent(state_size=4, action_size=3)\n",
    "batch_size = 32\n",
    "episodes = 500\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay(batch_size)\n",
    "    total_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31036355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Balance after testing: $102.35\n",
      "Total Profit: $-9897.65\n"
     ]
    }
   ],
   "source": [
    "# create a fresh environment instance for testing\n",
    "test_env = TradingEnvironment(data)\n",
    "state = test_env.reset()\n",
    "done = False\n",
    "\n",
    "# simulate a trading session using the trained agent\n",
    "while not done:\n",
    "    # always choose the best action (exploitation)\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = test_env.step(action)\n",
    "    state = next_state if next_state is not None else state\n",
    "\n",
    "final_balance = test_env.balance\n",
    "profit = final_balance - test_env.initial_balance\n",
    "print(f\"Final Balance after testing: ${final_balance:.2f}\")\n",
    "print(f\"Total Profit: ${profit:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
